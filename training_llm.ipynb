{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "K9HjEl-j2MFj",
        "fCBnAnMI2hL2"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use LLaMA-Factory to training"
      ],
      "metadata": {
        "id": "3Z8JnOfsM96W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j6lztgG0DZd",
        "outputId": "4e1666de-1592-40d2-c1eb-434729704bd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul 14 17:00:08 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8              13W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU configuration\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install LLaMA Factory dependencies\n",
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]\n",
        "!pip install rouge-chinese"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwPbM2w00NNN",
        "outputId": "e0d1bc54-8b81-4308-c3d4-8a2a5895c1b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 15452, done.\u001b[K\n",
            "remote: Counting objects: 100% (402/402), done.\u001b[K\n",
            "remote: Compressing objects: 100% (202/202), done.\u001b[K\n",
            "remote: Total 15452 (delta 249), reused 316 (delta 199), pack-reused 15050\u001b[K\n",
            "Receiving objects: 100% (15452/15452), 221.74 MiB | 15.81 MiB/s, done.\n",
            "Resolving deltas: 100% (11298/11298), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (4.41.2)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (2.20.0)\n",
            "Requirement already satisfied: accelerate>=0.30.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (0.32.1)\n",
            "Requirement already satisfied: peft>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (0.11.1)\n",
            "Requirement already satisfied: trl>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (0.9.6)\n",
            "Requirement already satisfied: gradio>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (4.38.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (1.11.4)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (0.1.99)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (0.7.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (3.20.3)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (0.30.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (2.8.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (0.111.0)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (2.1.2)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (3.7.1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (0.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (6.0.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (1.25.2)\n",
            "Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (0.43.1)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (2.3.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.9.5)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (5.3.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==1.1.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.27.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.10.6)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (9.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.0.9)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.5.2)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.0.7)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.1.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (11.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.3.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.3.dev0) (2.20.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.8.3.dev0) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.41.2->llamafactory==0.8.3.dev0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.41.2->llamafactory==0.8.3.dev0) (0.19.1)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl>=0.8.6->llamafactory==0.8.3.dev0) (0.8.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.3.dev0) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.3.dev0) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.8.3.dev0) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.8.3.dev0) (0.0.4)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.8.3.dev0) (5.10.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.8.3.dev0) (2.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.3.dev0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.3.dev0) (2.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette->llamafactory==0.8.3.dev0) (3.7.1)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.12.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.8.3.dev0) (2.6.1)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.8.3.dev0) (3.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.3.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette->llamafactory==0.8.3.dev0) (1.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (13.7.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0) (1.7.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.3.dev0) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.3.dev0) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.3.dev0) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.3.dev0) (0.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.8.3.dev0) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.19.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.1.2)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.8.3.dev0-0.editable-py3-none-any.whl size=20620 sha256=8e0193a9edc4f3bf89c775b13fc966f3106aaaf264cfbd883e0b9189e2e16329\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m87gg4bu/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: llamafactory\n",
            "  Attempting uninstall: llamafactory\n",
            "    Found existing installation: llamafactory 0.8.3.dev0\n",
            "    Uninstalling llamafactory-0.8.3.dev0:\n",
            "      Successfully uninstalled llamafactory-0.8.3.dev0\n",
            "Successfully installed llamafactory-0.8.3.dev0\n",
            "Requirement already satisfied: rouge-chinese in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the content of the NAME and AUTHOR variables.\n",
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"Wahoo-llama\"\n",
        "AUTHOR = \"SXM\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzbKDmEp0hW8",
        "outputId": "71db935e-f81c-44e7-c0e1-b852ca8b5722"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade huggingface_hub\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsN2k0Xl4a1V",
        "outputId": "f05ffbb1-4ed4-46f2-a90f-2ae217c5d9d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset and use it\n",
        "from datasets import load_dataset\n",
        "import hashlib\n",
        "\n",
        "sha1 = hashlib.sha1()\n",
        "ds = load_dataset(\"CAS-SIAT-XinHai/CPsyCoun\")\n",
        "\n",
        "dataset_bytes = json.dumps(ds['train'][:]).encode('utf-8')\n",
        "\n",
        "# Calculate SHA-1 hash value\n",
        "sha1.update(dataset_bytes)\n",
        "hash_value = sha1.hexdigest()\n",
        "\n",
        "print(\"SHA-1 Hash:\", hash_value)\n",
        "\n",
        "dataset_list = [entry for entry in ds['train']]\n",
        "\n",
        "\n",
        "with open('/content/LLaMA-Factory/data/psychology_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(dataset_list, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "dataset_info_path = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "dataset_info = {\n",
        "    \"psychology_data\": {\n",
        "        \"file_name\": \"psychology_data.json\",\n",
        "        \"file_sha1\": hash_value\n",
        "    }\n",
        "}\n",
        "\n",
        "try:\n",
        "    with open(dataset_info_path, 'r', encoding='utf-8') as f:\n",
        "        existing_info = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    existing_info = {}\n",
        "\n",
        "existing_info.update(dataset_info)\n",
        "\n",
        "with open(dataset_info_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(existing_info, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Updated {dataset_info_path} with new dataset info.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYmU2GRj6Bks",
        "outputId": "1c37d893-f413-44c7-d673-4d9d13bb9a72"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHA-1 Hash: 96c066885251d4f97be91f7fb9ab11867a0efc74\n",
            "Updated /content/LLaMA-Factory/data/dataset_info.json with new dataset info.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the running environment\n",
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk5FRhEC1ytQ",
        "outputId": "23684634-0eef-4364-ff63-f25f5bb26acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-07-14 17:01:59.314641: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 17:01:59.314704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 17:01:59.316145: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 17:01:59.323442: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 17:02:00.570504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://c6ddecc416f294c25f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "2024-07-14 17:04:10.260347: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 17:04:10.260409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 17:04:10.261783: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 17:04:11.562575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 17:04:19 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 17:04:20,209 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 17:04:20,210 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 17:04:20,210 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 17:04:20,210 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 17:04:20,711 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 17:04:20 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
            "07/14/2024 17:04:20 - INFO - llamafactory.data.loader - Loading dataset psychology_data.json...\n",
            "Generating train split: 3134 examples [00:00, 22293.18 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 3134/3134 [00:00<00:00, 4980.61 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 3134/3134 [00:14<00:00, 214.37 examples/s]\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 50287, 106155, 101600, 19361, 45893, 22649, 126763, 98739, 46281, 125011, 56386, 104586, 11571, 128009, 128006, 78191, 128007, 271, 46281, 114706, 110310, 40474, 12774, 103, 56386, 1811, 42421, 100815, 18904, 101067, 111230, 3922, 118899, 111573, 117293, 108803, 64209, 108966, 21388, 103192, 109798, 105187, 101879, 113385, 124376, 9554, 11571, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "听起来很有道理。那我们从哪里开始呢？<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "从了解你的情绪开始。试着回想一下，最近一次感到伤心或者失望的时候，是什么原因导致的？<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 46281, 114706, 110310, 40474, 12774, 103, 56386, 1811, 42421, 100815, 18904, 101067, 111230, 3922, 118899, 111573, 117293, 108803, 64209, 108966, 21388, 103192, 109798, 105187, 101879, 113385, 124376, 9554, 11571, 128009]\n",
            "labels:\n",
            "从了解你的情绪开始。试着回想一下，最近一次感到伤心或者失望的时候，是什么原因导致的？<|eot_id|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 17:04:39,064 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 17:04:39,067 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"shenzhi-wang/Llama3-8B-Chinese-Chat\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 17:04:39,211 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 17:04:39,215 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 17:04:39,217 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "Loading checkpoint shards:  75% 3/4 [01:02<00:20, 20.68s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 111, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\n",
            "    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/model/loader.py\", line 153, in load_model\n",
            "    model = AutoModelForCausalLM.from_pretrained(**init_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3754, in from_pretrained\n",
            "    ) = cls._load_pretrained_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4214, in _load_pretrained_model\n",
            "    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 887, in _load_state_dict_into_meta_model\n",
            "    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\", line 404, in set_module_tensor_to_device\n",
            "    new_value = value.to(device)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU \n",
            "2024-07-14 17:07:12.934998: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 17:07:12.935063: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 17:07:12.936438: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 17:07:14.273200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 17:07:22 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 17:07:22,484 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 17:07:22,484 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 17:07:22,485 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 17:07:22,485 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 17:07:22,941 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 17:07:22 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
            "07/14/2024 17:07:22 - INFO - llamafactory.data.loader - Loading dataset psychology_data.json...\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 50287, 106155, 101600, 19361, 45893, 22649, 126763, 98739, 46281, 125011, 56386, 104586, 11571, 128009, 128006, 78191, 128007, 271, 46281, 114706, 110310, 40474, 12774, 103, 56386, 1811, 42421, 100815, 18904, 101067, 111230, 3922, 118899, 111573, 117293, 108803, 64209, 108966, 21388, 103192, 109798, 105187, 101879, 113385, 124376, 9554, 11571, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "听起来很有道理。那我们从哪里开始呢？<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "从了解你的情绪开始。试着回想一下，最近一次感到伤心或者失望的时候，是什么原因导致的？<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 46281, 114706, 110310, 40474, 12774, 103, 56386, 1811, 42421, 100815, 18904, 101067, 111230, 3922, 118899, 111573, 117293, 108803, 64209, 108966, 21388, 103192, 109798, 105187, 101879, 113385, 124376, 9554, 11571, 128009]\n",
            "labels:\n",
            "从了解你的情绪开始。试着回想一下，最近一次感到伤心或者失望的时候，是什么原因导致的？<|eot_id|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 17:07:24,176 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 17:07:24,177 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"shenzhi-wang/Llama3-8B-Chinese-Chat\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 17:07:24,296 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--shenzhi-wang--Llama3-8B-Chinese-Chat/snapshots/f25f13cb2571e70e285121faceac92926b51e6f5/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 17:07:24,299 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 17:07:24,300 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "Loading checkpoint shards:  25% 1/4 [00:20<01:01, 20.38s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In order to upload the model to HuggingFace"
      ],
      "metadata": {
        "id": "K9HjEl-j2MFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "Y9Z2nscq2B-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "lzBQOLtI2Hfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "model_id = \"username/fine-tune-model\" #@param {type:\"string\"}\n",
        "export_folder_path = \"/content/LLaMA-Factory/export\" #@param {type:\"string\"}\n",
        "api.create_repo(model_id, private=True, exist_ok=True, repo_type=\"model\")\n",
        "api.upload_folder(\n",
        "    folder_path=export_folder_path,\n",
        "    repo_id=model_id,\n",
        ")"
      ],
      "metadata": {
        "id": "2Qm2Bz-c2ZjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save in Google Drive"
      ],
      "metadata": {
        "id": "fCBnAnMI2hL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yjLx9TlC2gVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Google_Drive_Folder = \"/content/drive/MyDrive/LLaMA-Factory/\" #@param {type:\"string\"}\n",
        "\n",
        "!cp -rf /content/LLaMA-Factory/saves {Google_Drive_Folder}"
      ],
      "metadata": {
        "id": "JiuDNUuH2ozl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}